---
title: "Lab 15: Ethics discussion"
author: "Solutions"
date: "November 11, 2021"
urlcolor: blue
linkcolor: blue
output:
 pdf_document:
# DO NOT CHANGE YAML BELOW THIS LINE  
  number_sections: yes
 # html_document:
 #  toc: yes
 #  toc_float: yes
classoption: fleqn
subparagraph: yes
header-includes:
 # LaTeX packages
 - \usepackage{fancyhdr,titling}\usepackage[compact]{titlesec}
 - \AtBeginEnvironment{quote}{\sffamily}
 - \renewcommand\familydefault{\sfdefault}
 # Question format (e.g., `Problem 3`)
 - > 
  \titleformat{\section}[hang]{\ifnum \value{section}>0 \newpage
                 \else \vspace{14pt} \fi
                 \sffamily\large}
                 {\hspace*{-9mm}Part \thesection}{4mm}{}[]
 # Subquestion format (e.g., `3.1`)
 - >
  \titleformat{\subsection}[hang]{\vspace{14pt}\sffamily\large}
                  {\hspace*{-9mm}\thesubsection}{4mm}{}[\vspace*{11pt}]
 # Page layout
 - >
  \pagestyle{fancy}\fancyhf{}
  \renewcommand{\headrulewidth}{0pt}
  \cfoot{\sffamily\thepage}
 # Header layout
 - >
  \lhead{\hspace*{-9mm}\sffamily\footnotesize 
      \copyright Brittney E. Bailey | STAT 231 | Lab}
 # Title layout
 - \pretitle{\hspace*{-9mm}\sffamily\footnotesize
       \copyright Brittney E. Bailey | STAT 231 | Lab
       \par \Large\bfseries \hspace*{-9mm}}
 - \posttitle{\\\hspace*{-9mm}\rule{7in}{2pt}}
 - \preauthor{\begin{flushright} \large} \postauthor{\end{flushright}}
 - \predate{\begin{flushright}\sffamily\footnotesize}
 - \postdate{\end{flushright}\vspace*{-33pt}}
 - \setlength{\droptitle}{-1in}
 # First page
 - >
  \AtBeginDocument{\sffamily\raggedright}
---

```{r setup, include = FALSE}
# load packages
library(tidyverse)
library(kableExtra)

library(ggnetwork)
library(igraph)

# set code chunk defaults
knitr::opts_chunk$set(tidy = F, # display code as typed
   size = "small", # slightly smaller code font
   message = FALSE,
   warning = FALSE,
   # center figures by default
   fig.align = "center",
   comment = "\t") 

# set black & white default plot theme
theme_set(theme_classic()) 

# improve digit and NA display 
options(scipen = 1, knitr.kable.NA = '')
```

# **About this lab** {-}
Choose one of the two algorithmic applications below that you are most interested in thinking about and discussing in class with a small group of your peers. Videos and/or readings are provided for each topic, as well as the questions you'll consider with your peers in class. I strongly encourage you to take notes on the questions as you watch/read/listen, so you can refer to them during your in-class discussion (you could take notes here, for example!).

> **Option 1:** [Predictive Policing and Predicting Recidivism](#policing)
> **Option 2:** [Predicting Financial Risk](#finance)

The resources and questions provided are designed to increase awareness around the complexity of the ethical considerations in data science, and around algorithms in particular.

Please complete this very brief [Google form](https://forms.gle/zuTR9QR6kNKvmFLc6) to specify you choice.

<!-- Part 1 ------------------------------------------------------------------->
# <!-- 1 -->**Small group discussion (25 minutes)** For this discussion, each student in the group will be in charge of managing, questioning, recording, or time keeping (if your group only has three people, then the Manager should also keep track of time). The purpose of establishing these roles is to support the overall collaborative learning process and to facilitate a richer discussion where all voices are heard. \newline\newline Unless you want a particular role, you can randomly assign using this sequence: Identify who in the group has the next birthday---they'll be the Manager. Identify who in the group has the next birthday after that---they'll be the Questioner. Identify who in the group has the next birthday after that---they'll be the Recorder. If you have a fourth person in your group, the remaining person will be the time keeper. Otherwise, the manager can also keep time.

**Manager:**: Makes sure all voices in the group are heard, and promotes active participation of all members. Makes sure the group remains focused during the conversation.

* Address group members by name and ensure that everyone contributes
* Help a student who hasn't talked much to enter the discussion
  * "[Name], what do you think about...?"
  * "I would like to hear what you think, (name)."
* Ask different members to read the conversation questions on a rotating basis

**Questioner:** Pushes back when the group comes to consensus too quickly without considering a number of options or points of view. The questioner makes sure that the group hears varied points of view, and that the group is not avoiding potentially rich areas of disagreement.

* Raise counter-arguments and constructive objections
* Introduce alternative explanations and perspectives
* Challenge group assumptions

**Recorder:** Takes notes on important thoughts expressed in the group. 

* Take notes on important thoughts expressed in the group
* Copy the relevant questions for your group from the "Lab 15 prep" document and share it with your group and the professor.
* Bullet points with important thoughts expressed while discussing the questions
 

**Time Keeper:** Monitors time and reminds group how much time is left. Moves the group along so they can discuss all questions.

* Take care of time management
* Keep an eye on the clock
  * "I think we need to focus on _________ so we get to discuss each of the questions."
  * "We have ______ minutes before we need to _________."
  

**Discussion Notes**

## <!-- 1 -->In the McKinsey article, interpretability and bias are two elements added to the framework for validating machine-learning-model risk. Consider the four definitions they offer for what constitutes fairness: demographic blindness, demographic parity, equal opportunity, and equal odds. Can you think of any other ways to measure fairness? Do you think it is possible for an algorithm to satisfy all notions of fairness at the same time?
* Ultimately the bank decides to interpret the data.
* Bias already exists
* Hard to balance between the different types of "fairness", each could introduce their own bias

## <!-- 2 -->In the WSJ article, the facial recognition technology developed by Ping An is used to gauge customers' health, which in turn can affect pricing of their insurance policy (e.g., discounts on the monthly premium if BMI < 30). In what ways is using facial recognition data and algorithms like this more effective (e.g., consider potential advantages to the consumer and the company) and more fair? In what ways is using data and facial recognition algorithms like this less effective and less fair? (Consider how you are defining effective and fair.)
* The judgement for appearance already exists, the software almost makes the process more transparent in what affects the subconscious judgements
* Not perfectly accurate
* BMI isn't nearly enough of an indicator for credit risk
* Redundant since physicals calculate BMI much more accurately anyways

## <!-- 3 -->In Buolamwini's facial recognition example, there seems to be a straightforward solution: expand the training set to include diverse individuals. What factors do you think have prevented this "straightforward solution" to being implemented more readily? 
* Technology that has these data skews against darker skinned people because of the distribution of technology
* There are simply less minorities in the databases that these programs are built

## <!-- 4 -->Do you think "expanding the training set" would solve any potential fairness issues in the context of Ping An's use of facial recognition? What about if the algorithm was going to determine whether or not you get into graduate school? Or whether or not you get the next job or summer internship that you apply to?
* The highest accuracy for BMI already exists, even expanding the training set it is only striving to hit that point
* Facial recognition is so easy to be tuned to factor in existing biases

<!-- Part 2 ------------------------------------------------------------------->
# <!-- 2 -->**College applications** In your first group, you unpacked some of the challenges around the use of algorithms in either the criminal justice system or the finance industry. Now let's think about an algorithmic application all of us can relate to directly: applying to college!

Consider the various sorts of information you had to submit in your college applications: demographics, standardized testing scores, high school transcripts, essays, etc. It used to be that humans in college admissions offices would sift through all this information and rank applicants based on the information they submitted. As technology and data science methods improved, more efficient ranking systems were created. Tech companies came along with admissions software that promised to improve results, cut costs, and "make admissions systems more equitable, by helping schools reduce unseen human biases that can impact admissions decisions." More recently, some schools have even accounted for information not explicitly submitted by prospective students: data from their web browsing histories and social media presence.

## <!-- 2.1 -->In a sense, the early ranking systems are still algorithms. But they were more interpretable and clear as compared to some of the black-box algorithms applied today. Which do you think is better for determining whether or not someone gets into college? Would you prefer to have a human analyze your college essay, or an algorithm? Explain your choice. (Consider how you're defining "better", and who you're considering "better" for.) 

* Humans could have a lot more unaccounted for biases but algorithms could miss so much subjective judgments 
* College essays are often emotional and current technology isn't advanced enough to interpret deeper data

## <!-- 2.2 -->Does it have to be a forced choice? Can you come up with an alternative?---so it's not a forced choice of "would you rather rely on the daily whims and biases of an (admissions officer, hiring employer, etc.) or rely on a computer algorithm based on biased data?" 

* 

## <!-- 2.3 -->A Brookings Institute report on [*Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms*](https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/) notes that "Fairness is a human, not a mathematical determination, grounded in shared ethical beliefs." Share with each other the different ways your first group defined "fairness" in algorithms. Can you agree upon what notions of fairness are (most) important to consider in this context? What are some ways algorithm operators and developers could ensure this fairness in an algorithm designed to aid college admissions officers in selecting applicants to accept?

