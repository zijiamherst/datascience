---
title: "Practice Set 5"
author: "Ziji Zhou"
date: "Due by 10pm ET on Friday"
urlcolor: blue
linkcolor: blue
output:
  pdf_document:
# DO NOT CHANGE YAML BELOW THIS LINE    
    number_sections: yes
classoption: fleqn
subparagraph: yes
header-includes:
  # LaTeX packages
  - \usepackage{fancyhdr,titling}\usepackage[compact]{titlesec}
  # Question format (e.g., `Problem 3`)
  - > 
    \titleformat{\section}[hang]{\ifnum \value{section}>0 \newpage
                                  \else \vspace{14pt} \fi
                                  \sffamily\large}
                                  {\hspace*{-9mm}Problem \thesection}{4mm}{}[]
  # Subquestion format (e.g., `3.1`)
  - >
    \titleformat{\subsection}[hang]{\vspace{14pt}\sffamily\large}
                                    {\hspace*{-9mm}\thesubsection}{4mm}{}[\vspace*{11pt}]
  # Page layout
  - >
    \pagestyle{fancy}\fancyhf{}
    \renewcommand{\headrulewidth}{0pt}
    \cfoot{\sffamily\thepage}
  # Header layout
  - >
    \lhead{\hspace*{-9mm}\sffamily\footnotesize 
            \copyright Brittney E. Bailey | STAT 231 | Problem set}
  # Title layout
  - \pretitle{\hspace*{-9mm}\sffamily\footnotesize
              \copyright Brittney E. Bailey | STAT 231 | Problem set
              \par \Large\bfseries \hspace*{-9mm}}
  - \posttitle{\\\hspace*{-9mm}\rule{7in}{2pt}}
  - \preauthor{\begin{flushright} \large} \postauthor{\end{flushright}}
  - \predate{\begin{flushright}\sffamily\footnotesize}
  - \postdate{\end{flushright}\vspace*{-33pt}}
  - \setlength{\droptitle}{-1in}
  # First page
  - >
    \AtBeginDocument{\sffamily\raggedright}
---

# **Practice Set Information** {-}

During the week, you will get further practice with the material by working through the Practice Set, a set of problems designed to give you practice beyond the examples produced in the text. 

You may work through these problems with peers, but all work must be completed by you (see the Honor Code in the syllabus) and you must indicate who you worked with below. 

Even then, the best approach here is to try the problems on your own before discussing them with peers, and then write your final solutions yourself.


## **GitHub Workflow** {-}

1. Before editing this file, verify you are working on the copy saved in *your* repo for the course (check the filepath and the project name in the top right corner). 

2. Before editing this file, make an initial commit of the file to your repo to add your copy of the problem set. 

3. Change your name at the top of the file and get started! 

4. You should *save, knit, and commit* the .Rmd file each time you've finished a question, if not more often. You should also *push* your commits back onto GitHub occasionally (you can do this after each commit). 

6. When you think you are done with the assignment, save the pdf as "*Name*\_*thisfilename*\_*date*.pdf" before committing and pushing (this is generally good practice but also helps me in those times where I need to download all student homework files).

## **Gradescope Upload** {-}

For each question (e.g., 3.1), allocate all pages associated with the specific question. If your work for a question runs onto a page that you did not select, you may not get credit for the work. If you do not allocate *any* pages when you upload your pdf, you may get a zero for the assignment.

You can resubmit your work as many times as you want before the deadline, so you should not wait until the last minute to submit some version of your work. Unexpected delays/crises that occur on the day the assignment is due do not warrant extensions (please submit whatever you have done to receive partial credit).

\clearpage
# **Practicing Academic Integrity** {-}

If you worked with others or used resources outside of provided course material (notes, textbook, etc) to complete this assignment, please acknowledge them below using a bulleted list. 
\normalfont

<!-- ~~~~~~~~~~~~~~~~ YOU MAY BEGIN EDITING BELOW THIS LINE ~~~~~~~~~~~~~~~~ -->

*I acknowledge the following individuals with whom I worked on this assignment:*

Name(s) and corresponding problem(s)

*

*I used the following sources to help complete this assignment:*

Source(s) and corresponding problem(s)

* 

```{r setup, include = FALSE}
# load packages
library(tidyverse)
library(kableExtra)
library(robotstxt)

# set code chunk defaults
knitr::opts_chunk$set(tidy = F, # display code as typed
                      size = "small", # slightly smaller code font
                      message = FALSE,
                      warning = FALSE,
                      comment = "\t") 

# set black & white default plot theme
theme_set(theme_classic()) 

# improve digit and NA display 
options(scipen = 1, knitr.kable.NA = '')
```

<!-- PROBLEM 1 ---------------------------------------------------------------->
# <!-- 1 -->**Justices of the Supreme Court of the United States**

## <!-- 1.1 -->Confirm that the following Wikipedia page allows automated scraping: https://en.wikipedia.org/wiki/List_of_justices_of_the_Supreme_Court_of_the_United_States

```{r}
url <- "https://en.wikipedia.org/wiki/List_of_justices_of_the_Supreme_Court_of_the_United_States"

# paths_allowed(url)
# SSL certificate not working
```

## <!-- 1.1 -->Go to the [List of Justices of the Supreme Court of the United States](https://en.wikipedia.org/wiki/List_of_justices_of_the_Supreme_Court_of_the_United_States). Create a new R script called "scrape-justices.R", and scrape the table of justices. Use `janitor::clean_names()` to tidy the names of the columns (do not do any additional wrangling beyond this for now), then write the final data frame to a csv file called "justices.csv" in the "data" folder using the `write_csv()` function. Commit and push both files in addition to this Rmd file when ready.

<!-- 
Add your code from justices.R to the code chunk below.

We do not want to re-run the code when we knit (we just want it to appear in the final pdf for the grader), so leave eval = FALSE in the chunk below.
-->
```{r, eval = FALSE}
library(tidyverse)
library(kableExtra)
library(robotstxt)
library(rvest)
library(purrr)

url <- "https://en.wikipedia.org/wiki/List_of_justices_of_the_Supreme_Court_of_the_United_States"

justice_data <- url %>%
  httr::GET(config = httr::config(ssl_verifypeer = FALSE)) %>% 
  read_html() %>%
  html_nodes("table") %>%
  html_table() %>%
  purrr::pluck(2) %>%
  janitor::clean_names() %>%
  write_csv("problem_sets/ps5/data/justices.csv")


```

## <!-- 1.2 -->Load "justices.csv" into this file using the `read_csv()` function. Then, modify the code below as needed and run the code to create the variable `tenure_length` (a numeric variable containing each justice's time spent on the bench). Create a visualization to show the distribution of tenure length of U.S. Supreme Court judges.  Interpret the plot.



```{r, fig.width = 3.6, fig.height = 2.4, fig.align = "center"}
justices <- read_csv("data/justices.csv")


# Run after loading your justices.csv file
justices <- justices %>%
  # Remove extra line that comes in at end of table
  filter(justice != "Justice") %>%
  
  # Some justices served <1 year; add "0 years," to make separating easier
  mutate(tenure_length = case_when(str_detect(tenure_length_d, "year") ~ tenure_length_d,
                                   TRUE ~ paste0("0 years, ", tenure_length_d))) %>% 
  separate(tenure_length, into = c("years_char", "days_char"),
           sep = ", ") %>% 
  mutate(tenure_years = parse_number(years_char) + (parse_number(days_char)/365)) %>%
  
  # Make date_confirmed_vote into date variable
  separate(date_confirmed_vote, into = c("date_confirmed", "vote"),
           sep = "\\(") %>%
  mutate(date_confirmed = lubridate::mdy(date_confirmed))

justices %>%
  ggplot(aes(x = tenure_years)) +
  geom_histogram(bins = 15) +
  labs(x = "Tenure in years",
       title = "US Justice Tenure Length")
```

This data shows a slightly skewed right data distribution of tenure lengths, most falling between 5-20 years of tenure.

<!-- PROBLEM 2 ---------------------------------------------------------------->
# <!-- 2 -->**Brainy Quotes** One theme of college (and life) is resilience. The code in the `scrape-resilience` chunk below scrapes a set of quotes returned from a search for "resilience" on BrainyQuote.com. The code in `display-quote` randomly selects one of those quotes and prints it.  When you're feeling frustrated, run that code chunk to randomly generate a quote to lift you up (or just make you laugh at the uselessness of the quote...some of them are pretty pathetic).

<!--
Do NOT remove  "eval = FALSE" option from this code chunk; you do not want it to evaluate it, i.e. scrape the site, every time you knit this file.
-->
```{r scrape-resilience, eval = FALSE}
quotes_url <- "https://www.brainyquote.com/topics/resilience-quotes"
robotstxt::paths_allowed(quotes_url)

quotes_html <- read_html(quotes_url)

quotes <- quotes_html %>%
  html_elements(".oncl_q") %>%
  html_text()

quotes <- quotes[which(quotes != " ")]

people <- quotes_html %>%
  html_elements(".oncl_a") %>%
  html_text()

# put in data frame with two variables (person and quote)
resilience_quotes <- tibble(person = people, quote = quotes) %>% 
  mutate(quote = str_remove_all(quote, "\n\n"),
         # Prep quotes for markdown display when `results = "asis"`
         display = paste0('> *"', as.character(quote), '"* --' , as.character(person)))

write_csv(resilience_quotes, "data/resilience_quotes.csv")
```

```{r display quote, results = "asis"}
resilience_quotes <- read_csv("data/resilience_quotes.csv")

slice_sample(resilience_quotes, n = 1) %>% 
  pull(display) %>% 
  cat()
```

## <!-- 2.1 -->Go to BrainyQuote.com and search a different topic or author that interests you.  Scrape the webpage returned from your search following the same code given above. Save your code in an R script called "scrape-quotes.R", and write the data frame to a csv called "quotes.csv" in the "data" folder. Be sure to push your R and csv files to your GitHub repo.

<!--
Add your code from scrape-quotes.R to the code chunk below.

We do not want to re-run the code when we knit (we just want it to appear in the final pdf for the grader), so leave eval = FALSE in the chunk below.
-->
```{r, eval = FALSE}
quotes_url <- "https://www.brainyquote.com/topics/wisdom-quotes"
robotstxt::paths_allowed(quotes_url)

quotes_html <- read_html(quotes_url)

quotes <- quotes_html %>%
  html_elements(".oncl_q") %>%
  html_text()

quotes <- quotes[which(quotes != " ")]

people <- quotes_html %>%
  html_elements(".oncl_a") %>%
  html_text()

# put in data frame with two variables (person and quote)
wisdom_quotes <- tibble(person = people, quote = quotes) %>% 
  mutate(quote = str_remove_all(quote, "\n\n"),
         # Prep quotes for markdown display when `results = "asis"`
         display = paste0('> *"', as.character(quote), '"* --' , as.character(person)))

write_csv(wisdom_quotes, "quotes.csv")
```

## <!-- 2.2 -->Load "quotes.csv" into this file using the `read_csv()` function.  Write code to select *three* of the quotes at random and print them (i.e., set `n = 3` in the `slice_sample()` function).

```{r, results = "asis"}
wisdom_quotes <- read_csv("data/quotes.csv")

slice_sample(resilience_quotes, n = 3) %>% 
  pull(display) %>% 
  cat()
```

<!--
Congrats! You've made it to the end. If you think you are done, read the instructions for how to do the final commit + push, this time including your renamed pdf, and upload your pdf to Gradescope.
-->
